{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Introduction](#introduction)\n",
    "    - [Addressing the Elephant in the Room](#addressing-the-elephant-in-the-room)\n",
    "    - [Linear and Logistic Regression in Brief](#linear-and-logistic-regression-in-brief)\n",
    "        - [Linear Regression](#linear-regression)\n",
    "        - [Logistic Regression](#logistic-regression)\n",
    "- [Methodology](#methodology)\n",
    "- [Implementation](#implementation)\n",
    "    - [Price Retrieval and Data Preprocessing](#price-retrieval-and-data-preprocessing)\n",
    "    - [Handling Zero Returns and Preparing for Binary Classification](#handling-zero-returns-and-preparing-for-binary-classification)\n",
    "    - [Feature Selection and Train-Test Split](#feature-selection-and-train-test-split)\n",
    "    - [Model Training](#model-training)\n",
    "    - [Predicting Movement and Assessing Accuracy of Predictions](#predicting-movement-and-assessing-accuracy-of-predictions)\n",
    "\n",
    "## Introduction\n",
    "The prediction of stock price movements remains a central challenge and a point of fascination in various disciplines, from finance and economics to machine learning. Complex methodologies, including deep learning and sophisticated time-series analyses, have advanced the field, often dominating the conversation. However, in the midst of this complexity, there is still a strong case for revisiting simpler models.\n",
    "\n",
    "In this study, we turn our attention to linear and logistic regression models to predict the directional movement of Apple Inc.'s stock price. Though these models may seem almost rudimentary compared to the multifaceted techniques now available, they offer valuable insights. Simplicity can provide clarity, interpretability, and a connection to foundational principles that might be lost in more intricate approaches.\n",
    "\n",
    "The goal here is not to overshadow the advanced models but to understand the basic ones, recognize their limitations, and appreciate their role in the broader context of predictive analytics. By focusing on linear and logistic regressions, this exploration serves as a reminder that sometimes, the most profound insights can be derived from the basics. It's an academic endeavor that reinforces the importance of foundational learning and the continuous pursuit of understanding, even in a field that is ever-evolving and increasingly complex.\n",
    "\n",
    "### Addressing the Elephant in the Room\n",
    "\n",
    "In financial time series data, autocorrelation can complicate modeling and forecasting. This statistical property, where a stock's price today might be closely related to its price in recent days or weeks, undermines the assumption that observations are independent. To address this challenge, our analysis will utilize a shorter timeframe for both training and testing. By limiting the period of analysis and employing logarithmic returns, we aim to diminish the effect of autocorrelation, thus creating a more reliable model. This decision enables a focus on more immediate relationships in the data, without the potential distortion caused by lingering long-term dependencies.\n",
    "\n",
    "### Linear and Logistic Regression in Brief\n",
    "#### Linear Regression\n",
    "Linear regression typically outputs continuous values. However, for our specific purpose of predicting the directional movement of stock prices (up or down), we need to convert these continuous predictions into binary form. We do this by applying a sign function to the continuous predictions. A positive prediction is translated into +1, representing an upward movement, while a negative prediction is translated into -1, representing a downward movement. This conversion allows us to compare the performance of the linear regression model directly with the logistic regression model, which inherently predicts binary outcomes. Linear regression can be generalized as such:\n",
    "\n",
    "$$y=\\beta{X} + \\alpha$$\n",
    "\n",
    "where $y$ is the predicted value, $\\beta$ is the coefficient for the independent variable, and $\\alpha$ is the y-intercept.\n",
    "\n",
    "#### Logistic Regression\n",
    "Logistic regression is designed to predict binary outcomes and outputs probabilities. In the context of our study, it is used to predict whether the stock price will move up or down. The model classifies the probabilities into two classes, typically using 0.5 as a threshold. A probability greater than 0.5 translates into a prediction of upward movement, while a probability less than or equal to 0.5 translates into a prediction of downward movement. Logistic regression can be generalized as such:\n",
    "\n",
    "$$p(y=1) = \\frac{1}{1 + e^{-(\\beta{X} + \\alpha)}}$$\n",
    "\n",
    "where $p(y=1)$ is the probability of that the dependent variable $y$ is 1 (e.g. the stock price goes up), $\\beta$ is the coefficient for the independent variable $X$, $\\alpha$ is the bias term, and $e$ is the base of the natural logarithm.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The methodology of this exploration is broken down as follows:\n",
    "\n",
    "1. **Retrieve Price Data for Apple Inc.**: Retrieve historical closing prices of Apple Inc. and selected market indices (e.g., VTI, DBC, ^VIX, AGG) from January 1<sup>st</sup>, 2022 to August 14<sup>th</sup>, 2023.\n",
    "\n",
    "2. **Preprocess Data**: Calculate the logarithmic daily returns of the price data to reduce the autocorrelation effect and create a more robust model.\n",
    "\n",
    "3. **Feature Selection**: The four market indices are chosen as proxies for the stock market (VTI), the commodities market (DBC), the bond market (AGG), and expected volatility (^VIX)\n",
    "\n",
    "4. **Split Data into Training and Testing Sets**: Divide the data into training and testing sets using a time-series split to maintain chronological order. A shorter timeframe is chosen for the analysis to ensure that the model is not confounded by long-term dependencies.\n",
    "\n",
    "5. **Train Linear and Logistic Regression Models**:\n",
    "    - **Linear Regression**: Fit a linear regression model using the training data, predicting the continuous log returns. Note here that the predictions—not the input—are converted into binary values based on the signage of the predictions (-1 for negative return predictions and +1 for positive return predictions)\n",
    "    - **Logistic Regression**: Fit a logistic regression model using the training data, predicting the binary directional movement (up/down).\n",
    "\n",
    "6. **Make Predictions**: Use the trained models to predict the stock price movement on the testing set.\n",
    "\n",
    "7. **Evaluate Model Accuracy**: Assess the models' accuracy by comparing the predicted directional movement with the actual movement.\n",
    "\n",
    "8. **Backtesting**: Create a backtesting framework to simulate trading based on the models' predictions. This includes logging transactions and calculating profits for different strategies.\n",
    "\n",
    "9. **Tune Logistic Regression Model**: Utilize techniques like Grid Search to find the optimal hyperparameters for the logistic regression model, retrain the model, and assess its performance.\n",
    "\n",
    "10. **Repeat Backtest with Tuned Logistic Regression Model**: Repeat the backtesting procedure using the signals generated by the tuned logistic regression model and compare results.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Price Retrieval and Data Preprocessing\n",
    "\n",
    "Prices for AAPL, VTI, DBC, AGG, and ^VIX were retrieved using the `yfinance` library and can be accessed [here](data/AAPL-VTI-DBC-VIX-AGG-prices-20220101-20230814.csv).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# Defining start and end dates.\n",
    "start = '2022-01-01'\n",
    "end = '2023-08-14'\n",
    "\n",
    "# Reading in the prices\n",
    "prices = pd.read_csv('data/AAPL-VTI-DBC-VIX-AGG-prices-20220101-20230814.csv', index_col=0, parse_dates=True)\n",
    "data = np.log(prices).diff()[1:]\n",
    "data.head()\n",
    "```\n",
    "\n",
    "![First five rows of `data`](img/data.png)\n",
    "\n",
    "### Handling Zero Returns and Preparing for Binary Classification\n",
    "In financial time series data, particularly when dealing with returns, it's possible to encounter zero values. These zeros can create complications when trying to predict binary outcomes, such as upward or downward stock price movements, as they introduce a third class that is neither upward nor downward.\n",
    "\n",
    "To ensure that our logistic regression model has only two classes to predict, we need to address these zero returns in our dataset. To address this, I replaced zeros with the mean of the non-zero values up to that point (i.e., the rolling mean). This technique preserves the overall distribution of the data while eliminating the third class:\n",
    "\n",
    "```python\n",
    "def replace_zeros_with_rolling_mean(series):\n",
    "    non_zero_values = series.replace(0, np.nan)\n",
    "    rolling_mean = non_zero_values.expanding().mean()\n",
    "    for idx, value in series.items():\n",
    "        if value == 0:\n",
    "            series.at[idx] = rolling_mean.at[idx]\n",
    "    return series\n",
    "\n",
    "data = data.apply(replace_zeros_with_rolling_mean)\n",
    "```\n",
    "\n",
    "### Feature Selection and Train-Test Split\n",
    "\n",
    "```python\n",
    "# Setting the X and y variables.\n",
    "X = data[indices]\n",
    "y_linear = data[stock].squeeze() # Continuous values for linear regression predictions.\n",
    "y_logistic = np.sign(y_linear) # Binary values for logistic regression predictions\n",
    "\n",
    "# Dividing the processed data into training and testing sets for linear and logistic regression models, ensuring chronological order.\n",
    "X_train, X_test, y_train_linear, y_test_linear = train_test_split(X, y_linear, shuffle=False, test_size=.2, random_state=42)\n",
    "_, _, y_train_logistic, y_test_logistic = train_test_split(X, y_logistic, shuffle=False, test_size=.2, random_state=42)\n",
    "```\n",
    "\n",
    "### Model Training\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "logistic_regression = LogisticRegression(random_state=42) # Set random state for reproducability.\n",
    "\n",
    "linear_regression.fit(X_train, y_train_linear)\n",
    "logistic_regression.fit(X_train, y_train_logistic);\n",
    "```\n",
    "\n",
    "### Predicting Movement and Assessing Accuracy of Predictions\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lin = np.sign(linear_regression.predict(X_test)) # Convert predictions into binary values\n",
    "y_pred_log = logistic_regression.predict(X_test)\n",
    "\n",
    "accuracy_lin = accuracy_score(np.sign(y_test_linear), y_pred_lin)\n",
    "accuracy_log = accuracy_score(y_test_logistic, y_pred_log)\n",
    "```\n",
    "![Accuracy Evaluation](img/acc1.png)\n",
    "\n",
    "It's interesting to see that the predictions of the linear regression model, even though it isn't inherently designed to classify, was higher than that of the logistic regression model. This outcome might seem counterintuitive, as logistic regression is specifically designed to handle binary classification tasks, while linear regression is generally used to predict continuous outcomes.\n",
    "\n",
    "However, this doesn't necessarily mean that linear regression is a superior model for predicting binary outcomes. Rather, it illustrates that the relationship between the features and the target variable in this specific dataset might be more linear in nature. The logistic regression model may also benefit from hyperparameter tuning, which could improve its performance - we'll see this for ourselves in the following section.\n",
    "\n",
    "### Hyperparameter Tuning for Logistic Regression\n",
    "\n",
    "```python\n",
    "import warnings # We import warnings to ignore ConvergenceWarnings.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the parameter grid.\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object.\n",
    "log_reg_grid = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5)\n",
    "\n",
    "# Fit to the data.\n",
    "log_reg_grid.fit(X_train, y_train_logistic)\n",
    "\n",
    "# Get the best parameters and model.\n",
    "tuned_params = log_reg_grid.best_params_\n",
    "log_reg_tuned = log_reg_grid.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_log_tuned = log_reg_tuned.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_log_tuned = accuracy_score(y_test_logistic, y_pred_log_tuned)\n",
    "\n",
    "print(f\"Tuned Logistic Regression Accuracy: {accuracy_log_tuned*100:,.2f}%\")\n",
    "print(\"Best Parameters:\", tuned_params)\n",
    "```\n",
    "<img src=\"img/acc2.png\" alt=\"Accuracy and Best Parameters for Tuned Logistic Regression\" width=\"50%\" height=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
